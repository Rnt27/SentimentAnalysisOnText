{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset-1 \n",
    "\n",
    "fetch_20newgroups\n",
    "\n",
    "\n",
    "multiclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#To load the datasets\n",
    "import os\n",
    "#to remove puntuation \n",
    "import string\n",
    "#To preprocess the datasets\n",
    "from nltk.corpus import stopwords #(tokonization)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re, string\n",
    "#packages for model selection (SVM)\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#GridSearh\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "#Time\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove = ('headers','footers','quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove = ('headers','footers','quotes'))\n",
    "#list(newsgroups.data)[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the target (the output and how many classes do we have)\n",
    "newsgroups_train.target_names\n",
    "newsgroups_test.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Spliting to (Training and Validatoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9051\n",
      "2263\n",
      "(7532,)\n",
      "[ 7  5  0 ...  9  6 15]\n",
      "[ 4  2 10 ...  6  9 17]\n"
     ]
    }
   ],
   "source": [
    "#Data Spliting randomly\n",
    "## Simple way to do split would be to use scikit-learn's `train_test_split` method\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(newsgroups_train.data, newsgroups_train.target, train_size=0.8, test_size=0.2)\n",
    "print(len(X_train))\n",
    "print(len(X_valid))\n",
    "y_test = newsgroups_test.target\n",
    "X_test = newsgroups_test.data\n",
    "print(y_test.shape)\n",
    "# train tesprint(len(y_train))\n",
    "# print(y_train[0])\n",
    "# print(X_train[1])\n",
    "print(y_test)\n",
    "print(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(file):\n",
    "    text_content = []\n",
    "    exclude = string.punctuation\n",
    "    exclude = exclude.replace(\"-\", \"\")\n",
    "    pattern = r\"[{}]\".format(exclude)\n",
    "\n",
    "    for data in file :\n",
    "        text = re.sub(r\"(<br\\s*/><br\\s*/>)\", \" \", str(data))\n",
    "        text = re.sub(pattern, \"\", str(text))\n",
    "        text_content.append(text.lower())\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "X_train1 = preprocessing(X_train)\n",
    "y_train = preprocessing(y_train)\n",
    "X_valid1 = preprocessing(X_valid)\n",
    "y_valid = preprocessing(y_valid)\n",
    "X_test1 = preprocessing(X_test)\n",
    "y_test = preprocessing(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFID\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "Train_set = tf_idf_vectorizer.fit_transform(X_train1)\n",
    "Valid_set = tf_idf_vectorizer.transform(X_valid1)\n",
    "Test_set = tf_idf_vectorizer.transform(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zalmhe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\zalmhe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 1857.495 s\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.003 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.003 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.003 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.003 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.682 (+/-0.023) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.003 (+/-0.000) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.715 (+/-0.002) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.683 (+/-0.023) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.711 (+/-0.011) for {'C': 1, 'kernel': 'linear'}\n",
      "0.722 (+/-0.002) for {'C': 10, 'kernel': 'linear'}\n",
      "0.706 (+/-0.012) for {'C': 100, 'kernel': 'linear'}\n",
      "0.705 (+/-0.012) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "predict time: 11.234 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.58696   0.70130   0.63905        77\n",
      "           1    0.73333   0.68750   0.70968       128\n",
      "          10    0.92661   0.75940   0.83471       133\n",
      "          11    0.81250   0.73585   0.77228       106\n",
      "          12    0.70370   0.65517   0.67857       116\n",
      "          13    0.87037   0.78992   0.82819       119\n",
      "          14    0.85714   0.66176   0.74689       136\n",
      "          15    0.69492   0.68908   0.69198       119\n",
      "          16    0.79787   0.75000   0.77320       100\n",
      "          17    0.86992   0.81679   0.84252       131\n",
      "          18    0.67059   0.58763   0.62637        97\n",
      "          19    0.66667   0.32609   0.43796        92\n",
      "           2    0.64228   0.63710   0.63968       124\n",
      "           3    0.70755   0.61983   0.66079       121\n",
      "           4    0.65625   0.65625   0.65625        96\n",
      "           5    0.78512   0.77236   0.77869       123\n",
      "           6    0.82222   0.67890   0.74372       109\n",
      "           7    0.27928   0.77500   0.41060       120\n",
      "           8    0.83544   0.62264   0.71351       106\n",
      "           9    0.78571   0.80000   0.79279       110\n",
      "\n",
      "    accuracy                        0.69156      2263\n",
      "   macro avg    0.73522   0.68613   0.69887      2263\n",
      "weighted avg    0.74060   0.69156   0.70441      2263\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zalmhe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 1851.186 s\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.050 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.050 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.050 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.050 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.520 (+/-0.025) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.050 (+/-0.000) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.690 (+/-0.007) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.520 (+/-0.025) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.685 (+/-0.007) for {'C': 1, 'kernel': 'linear'}\n",
      "0.641 (+/-0.006) for {'C': 10, 'kernel': 'linear'}\n",
      "0.598 (+/-0.004) for {'C': 100, 'kernel': 'linear'}\n",
      "0.598 (+/-0.003) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "predict time: 11.543 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.55000   0.71429   0.62147        77\n",
      "           1    0.74380   0.70312   0.72289       128\n",
      "          10    0.88034   0.77444   0.82400       133\n",
      "          11    0.79412   0.76415   0.77885       106\n",
      "          12    0.65323   0.69828   0.67500       116\n",
      "          13    0.87387   0.81513   0.84348       119\n",
      "          14    0.84348   0.71324   0.77291       136\n",
      "          15    0.69355   0.72269   0.70782       119\n",
      "          16    0.73529   0.75000   0.74257       100\n",
      "          17    0.87097   0.82443   0.84706       131\n",
      "          18    0.67021   0.64948   0.65969        97\n",
      "          19    0.65116   0.30435   0.41481        92\n",
      "           2    0.72115   0.60484   0.65789       124\n",
      "           3    0.65414   0.71901   0.68504       121\n",
      "           4    0.72340   0.70833   0.71579        96\n",
      "           5    0.79832   0.77236   0.78512       123\n",
      "           6    0.76190   0.73394   0.74766       109\n",
      "           7    0.42157   0.71667   0.53086       120\n",
      "           8    0.72816   0.70755   0.71770       106\n",
      "           9    0.71774   0.80909   0.76068       110\n",
      "\n",
      "    accuracy                        0.71542      2263\n",
      "   macro avg    0.72432   0.71027   0.71057      2263\n",
      "weighted avg    0.73105   0.71542   0.71703      2263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define the hyper-parameters\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    t0=time.time()\n",
    "    clf.fit(Train_set, y_train)\n",
    "    print (\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "    \n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    t1=time.time()\n",
    "    y_pred = clf.predict(Valid_set)\n",
    "    print (\"predict time:\", round(time.time()-t1, 3), \"s\")\n",
    "    print(metrics.classification_report(y_valid, y_pred, digits = 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Final (Selected) Model 1 for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 75.769 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.59794   0.63043   0.61376        92\n",
      "           1    0.69643   0.69643   0.69643       112\n",
      "          10    0.96078   0.80992   0.87892       121\n",
      "          11    0.91089   0.69697   0.78970       132\n",
      "          12    0.69421   0.62687   0.65882       134\n",
      "          13    0.86869   0.71074   0.78182       121\n",
      "          14    0.78070   0.74790   0.76395       119\n",
      "          15    0.67544   0.63115   0.65254       122\n",
      "          16    0.77451   0.73832   0.75598       107\n",
      "          17    0.82727   0.79130   0.80889       115\n",
      "          18    0.64286   0.59341   0.61714        91\n",
      "          19    0.56000   0.32558   0.41176        86\n",
      "           2    0.66667   0.67327   0.66995       101\n",
      "           3    0.65909   0.58000   0.61702       100\n",
      "           4    0.72917   0.64220   0.68293       109\n",
      "           5    0.79200   0.75573   0.77344       131\n",
      "           6    0.82828   0.66129   0.73543       124\n",
      "           7    0.26389   0.82609   0.40000       115\n",
      "           8    0.85227   0.65217   0.73892       115\n",
      "           9    0.84848   0.72414   0.78140       116\n",
      "\n",
      "    accuracy                        0.68272      2263\n",
      "   macro avg    0.73148   0.67570   0.69144      2263\n",
      "weighted avg    0.73954   0.68272   0.69902      2263\n",
      "\n",
      "predict time for test: 37.474 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.45741   0.45455   0.45597       319\n",
      "           1    0.59852   0.62468   0.61132       389\n",
      "          10    0.87572   0.75940   0.81342       399\n",
      "          11    0.78808   0.60101   0.68195       396\n",
      "          12    0.60725   0.51145   0.55525       393\n",
      "          13    0.76061   0.63384   0.69146       396\n",
      "          14    0.70112   0.63706   0.66755       394\n",
      "          15    0.66244   0.65578   0.65909       398\n",
      "          16    0.56986   0.57143   0.57064       364\n",
      "          17    0.83502   0.65957   0.73700       376\n",
      "          18    0.50000   0.37742   0.43015       310\n",
      "          19    0.35354   0.27888   0.31180       251\n",
      "           2    0.57841   0.57107   0.57471       394\n",
      "           3    0.58639   0.57143   0.57881       392\n",
      "           4    0.66978   0.55844   0.60907       385\n",
      "           5    0.74000   0.56203   0.63885       395\n",
      "           6    0.78592   0.71538   0.74899       390\n",
      "           7    0.24980   0.80556   0.38135       396\n",
      "           8    0.81356   0.60302   0.69264       398\n",
      "           9    0.80000   0.67506   0.73224       397\n",
      "\n",
      "    accuracy                        0.60117      7532\n",
      "   macro avg    0.64667   0.59135   0.60711      7532\n",
      "weighted avg    0.65585   0.60117   0.61621      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####Optimal Solution 1##############\n",
    "#kernel is linear, C=10.\n",
    "\n",
    "############ Valid+Train ##################\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "t0=time.time()\n",
    "clf = svm.SVC(kernel='linear',C = 10).fit(Train_set, y_train)\n",
    "print (\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "y_pred = clf.predict(Valid_set)\n",
    "print(metrics.classification_report(y_valid, y_pred, digits = 5))\n",
    "\n",
    "############ Test #################\n",
    "t2=time.time()\n",
    "y_pred = clf.predict(Test_set)\n",
    "print (\"predict time for test:\", round(time.time()-t2, 3), \"s\")\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Final (Selected) Model 2 for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 78.753 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.59223   0.66304   0.62564        92\n",
      "           1    0.65600   0.73214   0.69198       112\n",
      "          10    0.93519   0.83471   0.88210       121\n",
      "          11    0.88182   0.73485   0.80165       132\n",
      "          12    0.69841   0.65672   0.67692       134\n",
      "          13    0.85321   0.76860   0.80870       121\n",
      "          14    0.76667   0.77311   0.76987       119\n",
      "          15    0.67742   0.68852   0.68293       122\n",
      "          16    0.76190   0.74766   0.75472       107\n",
      "          17    0.79825   0.79130   0.79476       115\n",
      "          18    0.68235   0.63736   0.65909        91\n",
      "          19    0.58537   0.27907   0.37795        86\n",
      "           2    0.68182   0.74257   0.71090       101\n",
      "           3    0.72093   0.62000   0.66667       100\n",
      "           4    0.70536   0.72477   0.71493       109\n",
      "           5    0.87611   0.75573   0.81148       131\n",
      "           6    0.81982   0.73387   0.77447       124\n",
      "           7    0.42194   0.86957   0.56818       115\n",
      "           8    0.76577   0.73913   0.75221       115\n",
      "           9    0.81416   0.79310   0.80349       116\n",
      "\n",
      "    accuracy                        0.72205      2263\n",
      "   macro avg    0.73474   0.71429   0.71643      2263\n",
      "weighted avg    0.74206   0.72205   0.72449      2263\n",
      "\n",
      "predict time for test: 39.176 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.44759   0.49530   0.47024       319\n",
      "           1    0.56410   0.67866   0.61610       389\n",
      "          10    0.87568   0.81203   0.84265       399\n",
      "          11    0.79553   0.62879   0.70240       396\n",
      "          12    0.52764   0.53435   0.53097       393\n",
      "          13    0.75141   0.67172   0.70933       396\n",
      "          14    0.64218   0.68782   0.66422       394\n",
      "          15    0.70471   0.71357   0.70911       398\n",
      "          16    0.56266   0.60440   0.58278       364\n",
      "          17    0.79077   0.68351   0.73324       376\n",
      "          18    0.50196   0.41290   0.45310       310\n",
      "          19    0.41860   0.28685   0.34043       251\n",
      "           2    0.62097   0.58629   0.60313       394\n",
      "           3    0.62400   0.59694   0.61017       392\n",
      "           4    0.66854   0.61818   0.64238       385\n",
      "           5    0.80872   0.61013   0.69553       395\n",
      "           6    0.77083   0.75897   0.76486       390\n",
      "           7    0.40515   0.75505   0.52734       396\n",
      "           8    0.70408   0.69347   0.69873       398\n",
      "           9    0.73791   0.73048   0.73418       397\n",
      "\n",
      "    accuracy                        0.63834      7532\n",
      "   macro avg    0.64615   0.62797   0.63154      7532\n",
      "weighted avg    0.65430   0.63834   0.64084      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####Optimal Solution 2##############\n",
    "#kernel is radical basis function\n",
    "#C= 1000\n",
    "#gamma = 0.001\n",
    "############ Valid ##################\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "t0=time.time()\n",
    "clf = svm.SVC(kernel='rbf',C = 1000, gamma = 0.001 ).fit(Train_set, y_train)\n",
    "print (\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "\n",
    "y_pred = clf.predict(Valid_set)\n",
    "print(metrics.classification_report(y_valid, y_pred, digits = 5))\n",
    "t2=time.time()\n",
    "y_pred = clf.predict(Test_set)\n",
    "print (\"predict time for test:\", round(time.time()-t2, 3), \"s\")\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Selection_train(train_inp, y_tra, valid_inp, y_vald, clf, params):\n",
    "#def Model_Selection_train(train_inp, y_tra, valid_inp, y_vald, test_inp, y_tst, clf, params):\n",
    "    train_input = train_inp\n",
    "    valid_input = valid_inp\n",
    "    #test_input  = test_inp\n",
    "    \n",
    "    train_label = y_tra\n",
    "    valid_label = y_vald\n",
    "    #test_label  = y_tst\n",
    "    \n",
    "    if params != None :\n",
    "\n",
    "        combine_input = sparse.vstack([train_input, valid_input])\n",
    "        combine_label = np.concatenate((train_label, valid_label))\n",
    "        fold = [-1 for i in range(train_input.shape[0])] + [0 for i in range(valid_input.shape[0])]\n",
    "        #fold = 5\n",
    "        ps = PredefinedSplit(test_fold = fold)\n",
    "        clf = GridSearchCV(clf, params, cv=ps, refit=True)\n",
    "        clf.fit(combine_input, combine_label)\n",
    "    else:\n",
    "        clf.fit(train_input, train_label)\n",
    "        \n",
    "    best_param = None if params==None else clf.best_params_\n",
    "\n",
    "    f1_train = f1_score(train_label, clf.predict(train_input), average = 'micro')\n",
    "    f1_valid = f1_score(valid_label, clf.predict(valid_input), average = 'micro')\n",
    "    #f1_test = f1_score(test_label, clf.predict(test_input), average = 'micro')\n",
    "    Acc_train = accuracy_score(train_label, clf.predict(train_input), normalize = True)\n",
    "    Acc_valid = accuracy_score(valid_label, clf.predict(valid_input), normalize = True)\n",
    "    #Acc_test = accuracy_score(test_label, clf.predict(test_input), normalize = True)\n",
    "\n",
    "    return f1_train, f1_valid, Acc_train, Acc_valid, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'> Decision Tree \n",
      "(train, valid, test) =  (0.5488896254557507, 0.5475033141847105, 0.3777217206585236, 0.5488896254557507, 0.5475033141847105, 0.3777217206585236, {'max_features': 20000, 'max_leaf_nodes': 5000, 'min_samples_leaf': 13})\n",
      "best params = {'max_features': 20000, 'max_leaf_nodes': 5000, 'min_samples_leaf': 13}\n",
      "\n",
      "predict time: 8279.223 s\n"
     ]
    }
   ],
   "source": [
    "#fold is predefined\n",
    "#param = [{'criterion': ['gini'], 'splitter': ['best'] , 'min_samples_leaf': [np.arange(1, 20)], 'max_features': ['sqrt'], 'max_leaf_nodes': [1000 * i for i in range(3, 6)]}]\n",
    "#Decision Tree\n",
    "param = [{'max_features': [10000 * i for i in range(2, 9)], 'max_leaf_nodes': [1000 * i for i in range(1, 10)], 'min_samples_leaf': np.arange(1, 20)}]\n",
    "#param = [{'max_depth': [i for i in range(150, 200)], 'max_features': [10000 * i for i in range(1, 9)], 'max_leaf_nodes': [10000 * i for i in range(1, 10)], 'min_samples_leaf': np.arange(1, 20)}]\n",
    "t1=time.time()\n",
    "pred = Model_Selection_train(Train_set, y_train, Valid_set, y_valid, Test_set, y_test, DecisionTreeClassifier(), param)\n",
    "print(set, \"Decision Tree \\n(train, valid, test) = \", pred[:7])\n",
    "print(\"best params = {}\\n\".format(pred[6]))\n",
    "print (\"predict time:\", round(time.time()-t1, 3), \"s\")\n",
    "\n",
    "#param = [{'max_depth': [i for i in range(15, 20)], 'max_features': [10000 * i for i in range(2, 9)], 'max_leaf_nodes': [1000 * i for i in range(1, 10)], 'min_samples_leaf': np.arange(1, 20)}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'> Decision Tree \n",
      "(train, valid, test) =  (0.9741465031488233, 0.9765797613787008, 0.9741465031488233, 0.9765797613787008)\n",
      "best params = {'max_features': 60000, 'max_leaf_nodes': 9000, 'min_samples_leaf': 1}\n",
      "\n",
      "predict time: 8389.561 s\n"
     ]
    }
   ],
   "source": [
    "#fold is 5\n",
    "#Run it with no maximum depth (now im doing this)\n",
    "#param = [{'criterion': ['gini'], 'splitter': ['best'] , 'min_samples_leaf': [np.arange(1, 20)], 'max_features': ['sqrt'], 'max_leaf_nodes': [1000 * i for i in range(3, 6)]}]\n",
    "#Decision Tree\n",
    "param = [{'max_features': [10000 * i for i in range(2, 9)], 'max_leaf_nodes': [1000 * i for i in range(1, 10)], 'min_samples_leaf': np.arange(1, 20)}]\n",
    "#param = [{'max_depth': [i for i in range(150, 200)], 'max_features': [10000 * i for i in range(1, 9)], 'max_leaf_nodes': [10000 * i for i in range(1, 10)], 'min_samples_leaf': np.arange(1, 20)}]\n",
    "t1=time.time()\n",
    "pred = Model_Selection_train(Train_set, y_train, Valid_set,y_valid, DecisionTreeClassifier(), param)\n",
    "print(set, \"Decision Tree \\n(train, valid, test) = \", pred[:4])\n",
    "print(\"best params = {}\\n\".format(pred[4]))\n",
    "print (\"predict time:\", round(time.time()-t1, 3), \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Final (selected) model for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 23.462 s\n",
      "predict time: 0.003 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.25287   0.23913   0.24581        92\n",
      "           1    0.38462   0.35714   0.37037       112\n",
      "          10    0.61345   0.60331   0.60833       121\n",
      "          11    0.70000   0.53030   0.60345       132\n",
      "          12    0.32174   0.27612   0.29719       134\n",
      "          13    0.42029   0.47934   0.44788       121\n",
      "          14    0.45690   0.44538   0.45106       119\n",
      "          15    0.44954   0.40164   0.42424       122\n",
      "          16    0.35455   0.36449   0.35945       107\n",
      "          17    0.61386   0.53913   0.57407       115\n",
      "          18    0.27174   0.27473   0.27322        91\n",
      "          19    0.29412   0.17442   0.21898        86\n",
      "           2    0.39837   0.48515   0.43750       101\n",
      "           3    0.24138   0.21000   0.22460       100\n",
      "           4    0.41406   0.48624   0.44726       109\n",
      "           5    0.55469   0.54198   0.54826       131\n",
      "           6    0.45000   0.43548   0.44262       124\n",
      "           7    0.29954   0.56522   0.39157       115\n",
      "           8    0.55263   0.54783   0.55022       115\n",
      "           9    0.40385   0.36207   0.38182       116\n",
      "\n",
      "    accuracy                        0.42466      2263\n",
      "   macro avg    0.42241   0.41595   0.41490      2263\n",
      "weighted avg    0.43196   0.42466   0.42408      2263\n",
      "\n",
      "predict time for test: 0.012 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.18333   0.17241   0.17771       319\n",
      "           1    0.35149   0.36504   0.35813       389\n",
      "          10    0.53750   0.53885   0.53817       399\n",
      "          11    0.53097   0.45455   0.48980       396\n",
      "          12    0.26197   0.23664   0.24866       393\n",
      "          13    0.39257   0.37374   0.38292       396\n",
      "          14    0.42821   0.42386   0.42602       394\n",
      "          15    0.43855   0.39447   0.41534       398\n",
      "          16    0.31579   0.36264   0.33760       364\n",
      "          17    0.45676   0.44947   0.45308       376\n",
      "          18    0.19830   0.22581   0.21116       310\n",
      "          19    0.16201   0.11554   0.13488       251\n",
      "           2    0.38391   0.42386   0.40290       394\n",
      "           3    0.31856   0.29337   0.30544       392\n",
      "           4    0.40201   0.41558   0.40868       385\n",
      "           5    0.47619   0.37975   0.42254       395\n",
      "           6    0.53968   0.52308   0.53125       390\n",
      "           7    0.28864   0.46212   0.35534       396\n",
      "           8    0.48267   0.48995   0.48628       398\n",
      "           9    0.39560   0.36272   0.37845       397\n",
      "\n",
      "    accuracy                        0.38170      7532\n",
      "   macro avg    0.37724   0.37317   0.37322      7532\n",
      "weighted avg    0.38547   0.38170   0.38160      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############Valid####################\n",
    "t0=time.time()\n",
    "clf = DecisionTreeClassifier(max_features= 60000, max_leaf_nodes = 9000, min_samples_leaf = 1).fit(Train_set, y_train)\n",
    "print (\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "t1=time.time()\n",
    "y_pred = clf.predict(Valid_set)\n",
    "print (\"predict time:\", round(time.time()-t1, 3), \"s\")\n",
    "print(metrics.classification_report(y_valid, y_pred, digits = 5))\n",
    "############ Test #################\n",
    "t2=time.time()\n",
    "y_pred = clf.predict(Test_set)\n",
    "print (\"predict time for test:\", round(time.time()-t2, 3), \"s\")\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
