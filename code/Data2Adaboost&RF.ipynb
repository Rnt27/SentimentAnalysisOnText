{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 27\n",
    "# Dataset-2\n",
    "##### IMDB reviews\n",
    "\n",
    "##### Text Classification\n",
    "\n",
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import gensim\n",
    "import spacy\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from time import time\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the DataSet\n",
    "\n",
    "## Load Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/mnt/d/ml_p2/aclImdb/train/pos'\n",
    "all_trainpos = glob.glob(path + \"/*.txt\")\n",
    "\n",
    "path1 = r'/mnt/d/ml_p2/aclImdb/train/neg'\n",
    "all_trainneg = glob.glob(path1 + \"/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all pos\n",
    "output=''\n",
    "for file in all_trainpos:\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read().strip('\\n')\n",
    "        output += content + '\\t1\\n' #  This will use a placeholder of 1 for all labels.\n",
    "with open('train_pos.txt', 'w') as result:\n",
    "    result.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all neg\n",
    "output=''\n",
    "for file in all_trainneg:\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read().strip('\\n')\n",
    "        output += content + '\\t0\\n' #  This will use a placeholder of 0 for all labels.\n",
    "with open('train_neg.txt', 'w') as result:\n",
    "    result.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge neg and pos files\n",
    "filenames=['train_pos.txt', 'train_neg.txt']\n",
    "with open ('train.txt', 'w') as outfile:\n",
    "    for names in filenames:\n",
    "        with open(names) as infile:\n",
    "            outfile.write(infile.read())\n",
    "    outfile.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/mnt/d/ml_p2/aclImdb/test/pos'\n",
    "all_testpos = glob.glob(path + \"/*.txt\")\n",
    "\n",
    "path1 = r'/mnt/d/ml_p2/aclImdb/test/neg'\n",
    "all_testneg = glob.glob(path1 + \"/*.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all pos\n",
    "output=''\n",
    "for file in all_testneg:\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read().strip('\\n')\n",
    "        output += content + '\\t1\\n' #  This will use a placeholder of 1 for all labels.\n",
    "with open('test_pos.txt', 'w') as result:\n",
    "    result.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all pos and neg respectively\n",
    "output=''\n",
    "for file in all_testneg:\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read().strip('\\n')\n",
    "        output += content + '\\t0\\n' #  This will use a placeholder of 0 for all labels.\n",
    "with open('test_neg.txt', 'w') as result:\n",
    "    result.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge neg and pos files\n",
    "filenames=['test_pos.txt', 'test_neg.txt']\n",
    "with open ('test.txt', 'w') as outfile:\n",
    "    for names in filenames:\n",
    "        with open(names) as infile:\n",
    "            outfile.write(infile.read())\n",
    "    outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = open('train.txt')\n",
    "train = t.read()\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = open('test.txt')\n",
    "test = t.read()\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(file):\n",
    "    text_content = []\n",
    "    exclude = string.punctuation\n",
    "    exclude = exclude.replace(\"-\", \"\")\n",
    "    pattern = r\"[{}]\".format(exclude)\n",
    "\n",
    "    for data in file :\n",
    "        text = re.sub(r\"(<br\\s*/><br\\s*/>)\", \" \", str(data))\n",
    "        text = re.sub(pattern, \"\", str(text))\n",
    "        text_content.append(text.lower())\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocessing(train)\n",
    "test = preprocessing(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train, sep='\\t' ,names=['review', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.astype('str')\n",
    "sent = [row.split() for row in x['review']]\n",
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams:\n",
    "# We are using Gensim Phrases package to automatically detect common phrases (bigrams) from a list of sentences\n",
    "phrases = Phrases(sent, min_count=1, progress_per=100)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameter for w2v_model\n",
    "w2v_model = Word2Vec(min_count=2,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.01, \n",
    "                     min_alpha=0.03, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocab\n",
    "t = time()\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump('w2v_model', 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "w2v_model.wv.most_similar(positive=[\"movie\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorSpace Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = get_tmpfile(\"vectors.kv\")\n",
    "w2v_model.save(fname)\n",
    "word_vectors = KeyedVectors.load(fname, mmap='r')\n",
    "\n",
    "t=time()\n",
    "docs_vectors = pd.DataFrame()\n",
    "for doc in df['review']:\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc.split(' '):\n",
    "        if word in word_vectors:\n",
    "            try:\n",
    "                word_vec = word_vectors[word]\n",
    "                temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    doc_vector = temp.mean() \n",
    "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
    "\n",
    "\n",
    "print('Time to build: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "docs_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors.to_csv('docs_vectors.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Spliting to (Training and Validatoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(docs_vectors.drop('label', axis= 1), docs_vectors['label'],\n",
    "                                                    shuffle=True, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(test, sep='\\t' ,names=['review', 'label'])\n",
    "x_test = df1['reveiw']\n",
    "y_test = df1['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=20000,learning_rate = 0.3, random_state=42)\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('AdaBoost_model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test for test set we need to create a 300 dimention vector space\n",
    "test_pred = clf.predict(x_test)\n",
    "target_names = ['class 0 (neg)', 'class 1 (pos)']\n",
    "print(classification_report(y_test, test_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time()\n",
    "\n",
    "clf= RandomForestClassifier(n_estimators = 20000, max_features=None,\n",
    "                            max_depth=None, min_samples_split=2, min_samples_leaf=1, oob_score =True ,n_jobs = -2,\n",
    "                            bootstrap = True,random_state = 42 ) # criterion = 'gini'\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "print('Time to build: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('RandomForest_model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = clf.predict(x_test)\n",
    "target_names = ['class 0 (neg)', 'class 1 (pos)']\n",
    "print(classification_report(y_test, test_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
