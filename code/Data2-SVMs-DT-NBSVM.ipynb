{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 27\n",
    "\n",
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#To load the datasets\n",
    "import os\n",
    "#to remove puntuation \n",
    "import string\n",
    "#To preprocess the datasets\n",
    "from nltk.corpus import stopwords #(tokonization)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re, string\n",
    "#packages for model selection (SVM)\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#GridSearh\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "#Time\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Open_Files (path):\n",
    "    content = []\n",
    "    file_names = os.listdir(path)\n",
    "    for file in file_names:\n",
    "        t = path + file\n",
    "        content.append(open(t, encoding ='utf-8').read())  #English Text\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pos = Open_Files('./train/pos/')\n",
    "training_neg = Open_Files('./train/neg/')\n",
    "test_pos = Open_Files('./test/pos/')\n",
    "test_neg = Open_Files ('./test/neg/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Textual Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The function reads the file, removes punctuation marks (by translator), and puts everything in lower case. \n",
    "Coding BOW algorithm, the input will be multiple scentences and the output will be the vectors.\n",
    "- Step (1), tokenization: \n",
    "  - Discard some characters like punctuation marks.\n",
    "  - Remove Stopwords (do not give significant values to our model) by sorting a list of words. For example,\"is, the, a\" \n",
    "  - Robust implementation of stopwords (nltk library). It has a set of predefined words per language.\n",
    "- step (2), apply tokenization to all scentences. \n",
    "- step (3), build vocabulary and generate vectors.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(file):\n",
    "    text_content = []\n",
    "    exclude = string.punctuation\n",
    "    exclude = exclude.replace(\"-\", \"\")\n",
    "    pattern = r\"[{}]\".format(exclude)\n",
    "\n",
    "    for data in file :\n",
    "        text = re.sub(r\"(<br\\s*/><br\\s*/>)\", \" \", str(data))\n",
    "        text = re.sub(pattern, \"\", str(text))\n",
    "        text_content.append(text.lower())\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feature Extraction \n",
    "- extracts the top n frequent features with their respective ID. \n",
    "- writes output file for feature ID and count \n",
    "- writes output files for feature vectors for train, valid, test set \n",
    "\n",
    "This step later on (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Sbliting and Vectorzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Spliting and Labelization \n",
    "train_valid_cv= training_pos + training_neg\n",
    "train_valid_cv = preprocessing(train_valid_cv)\n",
    "training_set = training_pos[:10000] + training_neg[:10000]\n",
    "training_set = preprocessing(training_set)\n",
    "validation_set = training_pos [10000:] + training_neg[10000:]\n",
    "validation_set = preprocessing(validation_set)\n",
    "test = test_pos + test_neg\n",
    "test_set = preprocessing(test)\n",
    "#labelization\n",
    "y_train = np.append(np.ones(10000), np.zeros(10000))\n",
    "y_valid = np.append(np.ones(2500), np.zeros(2500))\n",
    "y_test = np.append(np.ones(12500), np.zeros(12500))\n",
    "y_cv = np.append(np.ones(12500), np.zeros(12500))\n",
    "#y_crossvalidation = np.append(np.ones(12500), np.zeros(12500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tifd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "train_set = tf_idf_vectorizer.fit_transform(training_set)\n",
    "train_valid_set = tf_idf_vectorizer.fit_transform(train_valid_cv)\n",
    "valid_set = tf_idf_vectorizer.transform(validation_set)\n",
    "test_set = tf_idf_vectorizer.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 95192)\n",
      "(20000, 84708)\n",
      "(25000, 95192)\n",
      "(25000, 95192)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(valid_set.shape)\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)\n",
    "print(train_valid_set.shape)\n",
    "print(y_cv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch: SVM with different Kernel Functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the hyper-parameters\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    t0=time.time()\n",
    "    clf.fit(train_set, y_train)\n",
    "    print (\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "    \n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    #print()\n",
    "    #t1=time.time()\n",
    "    #y_pred = clf.predict(tes_set)\n",
    "    #print (\"predict time:\", round(time.time()-t1, 3), \"s\")\n",
    "    #print(metrics.classification_report(y_tes, y_pred, digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Selection: Tunning Hyperparameters:\n",
    "\n",
    "Function: \n",
    "- the objective is to find the best hyperparameter for the model (clf-Sklearn)\n",
    "-Then refit the model with the paramters\n",
    "-Predict train and valid \n",
    "-Calculate f1_score and accuracy\n",
    "-Returns: f1_score train and f1_score valid\n",
    "-Inputs: set (dataset), clf (model), params (tunning hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_Selection_train(train_inp, y_tra, valid_inp, y_vald, test_inp, y_tst, clf, params):\n",
    "\n",
    "    train_input = train_inp\n",
    "    valid_input = valid_inp\n",
    "    test_input  = test_inp\n",
    "    \n",
    "    train_label = y_tra\n",
    "    valid_label = y_vald\n",
    "    test_label  = y_tst\n",
    "    \n",
    "    if params != None :\n",
    "        '''cross-validation  \n",
    "        1) First, we combine training and validation set togther\n",
    "        2) Second, we feed the split into GridSearchCV (with cross validation)\n",
    "        3) Third, we calculate the accuracy on training, validation, and test.\n",
    "        '''\n",
    "        combine_input = sparse.vstack([train_input, valid_input])\n",
    "        combine_label = np.concatenate((train_label, valid_label))\n",
    "        fold = [-1 for i in range(train_input.shape[0])] + [0 for i in range(valid_input.shape[0])]\n",
    "        ps = PredefinedSplit(test_fold = fold)\n",
    "        clf = GridSearchCV(clf, params, cv=ps, refit=True)\n",
    "        clf.fit(combine_input, combine_label)\n",
    "    else:\n",
    "        clf.fit(train_input, train_label)\n",
    "        \n",
    "    best_param = None if params==None else clf.best_params_\n",
    "\n",
    "    f1_train = f1_score(train_label, clf.predict(train_input), average = 'micro')\n",
    "    f1_valid = f1_score(valid_label, clf.predict(valid_input), average = 'micro')\n",
    "    f1_test = f1_score(test_label, clf.predict(test_input), average = 'micro')\n",
    "    Acc_train = accuracy_score(train_label, clf.predict(train_input), normalize = True)\n",
    "    Acc_valid = accuracy_score(valid_label, clf.predict(valid_input), normalize = True)\n",
    "    Acc_test = accuracy_score(test_label, clf.predict(test_input), normalize = True)\n",
    "\n",
    "    return f1_train, f1_valid, f1_test, Acc_train, Acc_valid, Acc_test, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search (1) for Decision Trees \n",
    "#Hyperparameters : \n",
    "# max-features = 80000\n",
    "# max-depth = [i for i in range(15, 20)]\n",
    "# max-leaf-nodes = [1000 * i for i in range(3, 6)] \n",
    "# min-samples-leaf = np.arange(1, 20)\n",
    "\n",
    "param = [{'max_depth': [i for i in range(15, 20)], 'max_features': [80000], 'max_leaf_nodes': [1000 * i for i in range(3, 6)], 'min_samples_leaf': np.arange(1, 20)}]\n",
    "t1=time.time()\n",
    "pred = Model_Selection_train(train_set, y_train, valid_set, y_valid, test_set, y_test, DecisionTreeClassifier(), param)\n",
    "print(set, \"Decision Tree \\n(train, valid, test) = \", pred[:6])\n",
    "print(\"best params = {}\\n\".format(pred[6]))\n",
    "print (\"predict time:\", round(time.time()-t1, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search (2) for Decision Trees \n",
    "#Hyperparameters : \n",
    "# max-features = 80000\n",
    "# max-depth = [i for i in range(15, 20)]\n",
    "# max-leaf-nodes = [1000 * i for i in range(3, 6)] \n",
    "# min-samples-leaf = np.arange(1, 20)\n",
    "\n",
    "param = [{'max_depth': [i for i in range(15, 20)], 'max_features': [1000 * i for i in range(2, 7)], 'max_leaf_nodes': [1000 * i for i in range(3, 6)], 'min_samples_leaf': np.arange(1, 20)}]\n",
    "pred = Model_Selection_train(train_set, y_train, valid_set, y_valid, test_set, y_test, DecisionTreeClassifier(), param)\n",
    "print(set, \"Decision Tree \\n(train, valid, test) = \", pred[:6])\n",
    "print(\"best params = {}\\n\".format(pred[6]))\n",
    "\n",
    "#based on a reference the minimum samples leaf (1, 20)(refrence: â€œAn empirical study on hyperparameter tuning of decision trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Final (selected) model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 451.324 s\n",
      "predict time: 412.034 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.92810   0.91280   0.92038     12500\n",
      "         1.0    0.91421   0.92928   0.92169     12500\n",
      "\n",
      "    accuracy                        0.92104     25000\n",
      "   macro avg    0.92115   0.92104   0.92103     25000\n",
      "weighted avg    0.92115   0.92104   0.92103     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############ Train+ Valid ##################\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "t0=time.time()\n",
    "clf = svm.SVC(kernel='rbf',C = 100, gamma = 0.001 ).fit(train_valid_set, y_cv)\n",
    "print (\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "t1=time.time()\n",
    "y_pred = clf.predict(train_valid_set)\n",
    "print (\"predict time:\", round(time.time()-t1, 3), \"s\")\n",
    "print(metrics.classification_report(y_cv, y_pred, digits = 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time for test: 404.358 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.88513   0.87840   0.88175     12500\n",
      "         1.0    0.87932   0.88600   0.88265     12500\n",
      "\n",
      "    accuracy                        0.88220     25000\n",
      "   macro avg    0.88222   0.88220   0.88220     25000\n",
      "weighted avg    0.88222   0.88220   0.88220     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############ Test #################\n",
    "t2=time.time()\n",
    "y_pred = clf.predict(test_set)\n",
    "print (\"predict time for test:\", round(time.time()-t2, 3), \"s\")\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 16.647 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.83802   0.77648   0.80608     12500\n",
      "         1.0    0.79177   0.84992   0.81982     12500\n",
      "\n",
      "    accuracy                        0.81320     25000\n",
      "   macro avg    0.81490   0.81320   0.81295     25000\n",
      "weighted avg    0.81490   0.81320   0.81295     25000\n",
      "\n",
      "predict time for test: 0.041 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.73908   0.68208   0.70944     12500\n",
      "         1.0    0.70484   0.75920   0.73101     12500\n",
      "\n",
      "    accuracy                        0.72064     25000\n",
      "   macro avg    0.72196   0.72064   0.72022     25000\n",
      "weighted avg    0.72196   0.72064   0.72022     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############Train+Valid################\n",
    "t0=time.time()\n",
    "clfdt = DecisionTreeClassifier(max_depth= 18, max_features= 80000, max_leaf_nodes= 3000, min_samples_leaf= 8 ).fit(train_valid_set, y_cv)\n",
    "print (\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "t1=time.time()\n",
    "y_pred = clfdt.predict(train_valid_set)\n",
    "print(metrics.classification_report( y_cv, y_pred, digits = 5))\n",
    "#########Test######################\n",
    "t2=time.time()\n",
    "y_pred = clfdt.predict(test_set)\n",
    "print (\"predict time for test:\", round(time.time()-t2, 3), \"s\")\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Model 1 #############################\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import sparse\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=True, n_jobs=1, beta = 0.75):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "        self.beta = beta\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        #y = y.values\n",
    "        #x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LinearSVC(C=self.C, dual=self.dual).fit(x_nb, y)\n",
    "        w = np.append(self._clf.coef_, self._clf.intercept_)\n",
    "        wbar = np.abs(w).mean(axis=0)\n",
    "        self._clf.coef_ = self.beta * wbar + (1 - self.beta) * self._clf.coef_\n",
    "        self._clf.intercept_ = self.beta * wbar + (1 - self.beta) * self._clf.intercept_\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 1.05 s\n",
      "predict time: 0.169 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.95507   0.93864   0.94678     12500\n",
      "         1.0    0.93968   0.95584   0.94769     12500\n",
      "\n",
      "    accuracy                        0.94724     25000\n",
      "   macro avg    0.94737   0.94724   0.94724     25000\n",
      "weighted avg    0.94737   0.94724   0.94724     25000\n",
      "\n",
      "predict time: 0.166 s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.89526   0.88688   0.89105     12500\n",
      "         1.0    0.88793   0.89624   0.89207     12500\n",
      "\n",
      "    accuracy                        0.89156     25000\n",
      "   macro avg    0.89159   0.89156   0.89156     25000\n",
      "weighted avg    0.89159   0.89156   0.89156     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#########Train + Valid ################################\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "t0=time.time()\n",
    "nbSVM = NbSvmClassifier(C=1, beta =0.25).fit(train_valid_set, y_cv)\n",
    "print (\"training time:\", round(time.time()-t0, 3), \"s\")\n",
    "\n",
    "t1=time.time()\n",
    "y_pred = nbSVM.predict(train_valid_set)\n",
    "print (\"predict time:\", round(time.time()-t1, 3), \"s\")\n",
    "print(metrics.classification_report(y_cv, y_pred, digits = 5))\n",
    "\n",
    "################ Test #######################################\n",
    "t1=time.time()\n",
    "y_pred = nbSVM.predict(test_set)\n",
    "print (\"predict time:\", round(time.time()-t1, 3), \"s\")\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
